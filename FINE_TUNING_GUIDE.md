# üìö Guide : Fine-tuning et Int√©gration de Donn√©es

## üéØ M√©thodes d'Apprentissage Actuelles

Votre IA personnelle apprend de **3 fa√ßons** :

### 1Ô∏è‚É£ **Ingestion de Documents** (M√©moire Vectorielle)

Les documents sont stock√©s dans la m√©moire vectorielle (FAISS) et utilis√©s comme contexte lors des r√©ponses.

**Utilisation :**
```bash
# Dans l'interface CLI
ingest mon_document.txt
ingest data.json
ingest documentation.md
```

**Formats support√©s :**
- `.txt` - Fichiers texte
- `.md` - Markdown
- `.json` - JSON (extraction automatique des strings)

**Comment √ßa fonctionne :**
- Le document est d√©coup√© en chunks de ~500 caract√®res
- Chaque chunk est converti en embedding vectoriel
- Stock√© dans FAISS pour recherche s√©mantique
- R√©cup√©r√© automatiquement lors des conversations pertinentes

### 2Ô∏è‚É£ **Enseignement Explicite** (Haute Priorit√©)

Les informations enseign√©es explicitement sont stock√©es avec haute importance.

```bash
teach Python utilise l'indentation pour les blocs de code
teach Mon nom est Alice et j'aime la programmation
```

### 3Ô∏è‚É£ **Apprentissage par Interaction** (SQLite)

Chaque interaction est enregistr√©e pour analyse future.

```bash
talk Bonjour, comment vas-tu ?
correct Python est rapide | Python est interpr√©t√© donc g√©n√©ralement plus lent
```

---

## üîß Fine-tuning du Mod√®le LLM

**‚ö†Ô∏è Important :** Le syst√®me actuel utilise la **m√©moire contextuelle** (RAG) plut√¥t que le fine-tuning du mod√®le. C'est plus flexible et ne n√©cessite pas de r√©entra√Æner le mod√®le.

### Option 1 : Fine-tuning avec Ollama (Recommand√©)

Ollama supporte le fine-tuning via Modelfiles. Voici comment :

#### √âtape 1 : Pr√©parer vos donn√©es

Cr√©ez un fichier `training_data.txt` au format conversation :

```
Human: Bonjour
Assistant: Bonjour ! Comment puis-je vous aider aujourd'hui ?

Human: Quel est ton nom ?
Assistant: Je suis votre IA personnelle. Vous pouvez me donner un nom si vous le souhaitez.

Human: Appelle-toi Alice
Assistant: Parfait, je m'appelle d√©sormais Alice. Ravi de vous rencontrer !
```

#### √âtape 2 : Cr√©er un Modelfile

Cr√©ez `Modelfile` :

```
FROM llama3

SYSTEM """Tu es une IA personnelle autonome et locale.
Tu d√©veloppes une personnalit√© unique bas√©e sur tes interactions.
Tu es curieux, attentif et empathique."""

TEMPLATE """{{ .System }}

User: {{ .Prompt }}
Assistant: {{ .Response }}"""
```

#### √âtape 3 : Fine-tuner avec Ollama

```bash
# Cr√©er un mod√®le personnalis√©
ollama create my-ai -f Modelfile

# Fine-tuner avec vos donn√©es (n√©cessite ollama>=0.1.7)
ollama train my-ai --data training_data.txt
```

#### √âtape 4 : Utiliser le mod√®le fine-tun√©

Modifiez `config.py` :
```python
DEFAULT_MODEL = "my-ai"  # Votre mod√®le personnalis√©
```

### Option 2 : Fine-tuning via API Python

Cr√©ez un script `fine_tune.py` :

```python
import ollama
from pathlib import Path

def fine_tune_from_data(data_file: Path, model_name: str = "my-ai"):
    """Fine-tune un mod√®le Ollama √† partir d'un fichier de donn√©es."""
    
    # Lire les donn√©es
    with open(data_file, 'r', encoding='utf-8') as f:
        training_data = f.read()
    
    # Cr√©er le mod√®le personnalis√©
    ollama.create(
        model=model_name,
        modelfile=f"""
FROM llama3

SYSTEM \"\"\"Tu es une IA personnelle autonome.
Tu apprends de chaque interaction.
Tu d√©veloppes une personnalit√© unique.\"\"\"
"""
    )
    
    # Fine-tuner (si support√© par votre version d'Ollama)
    # Note: Cette fonctionnalit√© peut varier selon la version
    print(f"Mod√®le {model_name} cr√©√©. Utilisez 'ollama train' pour le fine-tuner.")
```

### Option 3 : Am√©lioration du Syst√®me Actuel (Recommand√©)

Au lieu de fine-tuner le mod√®le, vous pouvez am√©liorer le syst√®me de m√©moire et de prompts :

1. **Enrichir la m√©moire** avec plus de documents
2. **Am√©liorer les prompts syst√®me** dans `reasoning/prompt_builder.py`
3. **Ajuster la personnalit√©** dans `config.py`

---

## üìä Int√©gration de Donn√©es en Masse

### M√©thode 1 : Script Python pour Ingestion Multiple

Cr√©ez `scripts/batch_ingest.py` :

```python
from pathlib import Path
from learning.document_ingest import DocumentIngest
from memory.long_term import LongTermMemory
import config

def batch_ingest(directory: Path, importance: float = 0.6):
    """Ing√®re tous les fichiers d'un r√©pertoire."""
    memory = LongTermMemory()
    ingest = DocumentIngest(memory)
    
    total = 0
    for file_path in directory.rglob("*"):
        if file_path.is_file():
            try:
                if file_path.suffix == '.json':
                    count = ingest.ingest_json(file_path, importance)
                elif file_path.suffix in ['.txt', '.md']:
                    count = ingest.ingest_text_file(file_path, importance)
                else:
                    continue
                
                total += count
                print(f"‚úÖ {file_path.name}: {count} souvenirs")
            except Exception as e:
                print(f"‚ùå {file_path.name}: {e}")
    
    print(f"\nüìä Total: {total} souvenirs cr√©√©s")

if __name__ == "__main__":
    data_dir = Path("data/documents")
    batch_ingest(data_dir)
```

### M√©thode 2 : Format JSON Structur√©

Cr√©ez `data/training_data.json` :

```json
{
  "conversations": [
    {
      "user": "Bonjour",
      "assistant": "Bonjour ! Comment puis-je vous aider ?",
      "importance": 0.8,
      "tags": ["greeting", "basic"]
    },
    {
      "user": "Qu'est-ce que Python ?",
      "assistant": "Python est un langage de programmation interpr√©t√©, orient√© objet et de haut niveau.",
      "importance": 0.9,
      "tags": ["python", "programming"]
    }
  ],
  "facts": [
    {
      "text": "Python utilise l'indentation pour d√©limiter les blocs de code",
      "importance": 0.9,
      "category": "programming"
    }
  ]
}
```

Puis ing√©rez :
```bash
ingest data/training_data.json
```

### M√©thode 3 : CSV pour Donn√©es Structur√©es

Cr√©ez `scripts/ingest_csv.py` :

```python
import csv
from pathlib import Path
from learning.document_ingest import DocumentIngest
from memory.long_term import LongTermMemory

def ingest_csv(csv_file: Path):
    """Ing√®re un fichier CSV."""
    memory = LongTermMemory()
    
    with open(csv_file, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            # Combiner les colonnes pertinentes
            text = f"{row.get('question', '')} | {row.get('answer', '')}"
            importance = float(row.get('importance', 0.7))
            
            memory.store_memory(
                text=text,
                importance=importance,
                metadata={"source": str(csv_file), "type": "csv"}
            )
    
    print(f"‚úÖ CSV ing√©r√©: {csv_file}")

if __name__ == "__main__":
    ingest_csv(Path("data/training.csv"))
```

---

## üéØ Meilleures Pratiques

### 1. **Organisation des Donn√©es**

```
data/
‚îú‚îÄ‚îÄ documents/          # Documents √† ing√©rer
‚îÇ   ‚îú‚îÄ‚îÄ knowledge_base/
‚îÇ   ‚îú‚îÄ‚îÄ conversations/
‚îÇ   ‚îî‚îÄ‚îÄ facts/
‚îú‚îÄ‚îÄ training/          # Donn√©es pour fine-tuning
‚îÇ   ‚îú‚îÄ‚îÄ conversations.txt
‚îÇ   ‚îî‚îÄ‚îÄ Modelfile
‚îî‚îÄ‚îÄ custom/            # Donn√©es personnalis√©es
```

### 2. **Hi√©rarchie d'Importance**

- **0.9-1.0** : Informations critiques, corrections importantes
- **0.7-0.8** : Enseignements explicites, faits importants
- **0.5-0.6** : Documents g√©n√©raux, conversations normales
- **0.3-0.4** : Contexte secondaire

### 3. **Format des Donn√©es**

Pour de meilleurs r√©sultats :
- **Conversations** : Format "Human: ... Assistant: ..."
- **Facts** : Phrases compl√®tes et claires
- **Documents** : Structure claire avec titres/sections

---

## üöÄ Exemple Complet

### √âtape 1 : Pr√©parer vos donn√©es

```bash
# Cr√©er le r√©pertoire
mkdir -p data/documents

# Ajouter vos fichiers
cp mes_documents/*.txt data/documents/
cp mes_documents/*.md data/documents/
```

### √âtape 2 : Ing√©rer en masse

```python
# Dans Python
from scripts.batch_ingest import batch_ingest
from pathlib import Path

batch_ingest(Path("data/documents"), importance=0.7)
```

### √âtape 3 : V√©rifier

```bash
# Dans l'interface CLI
remember
status
```

### √âtape 4 : Tester

```bash
talk Parle-moi de [sujet de vos documents]
```

---

## üìù Notes Importantes

1. **M√©moire vs Fine-tuning** :
   - La m√©moire (RAG) est plus flexible et rapide
   - Le fine-tuning modifie le mod√®le de fa√ßon permanente
   - Combinez les deux pour de meilleurs r√©sultats

2. **Performance** :
   - Plus de donn√©es = meilleure compr√©hension
   - Mais attention √† la qualit√© > quantit√©
   - Privil√©giez des donn√©es pertinentes et bien format√©es

3. **Maintenance** :
   - V√©rifiez r√©guli√®rement la m√©moire avec `remember`
   - Supprimez les souvenirs obsol√®tes avec `forget <id>`
   - Surveillez l'espace disque (FAISS peut devenir volumineux)

---

**üí° Conseil :** Commencez par l'ingestion de documents (m√©thode la plus simple), puis explorez le fine-tuning si vous avez besoin de comportements tr√®s sp√©cifiques.
