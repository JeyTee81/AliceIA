# ‚ö° Optimisation des Performances

Guide pour am√©liorer la vitesse de r√©ponse de l'IA tout en pr√©servant l'apprentissage continu.

## üéØ Optimisations Appliqu√©es (Sans Limiter l'Apprentissage)

### 1. Feedback Visuel Am√©lior√©

- Indicateur de progression anim√© pendant la g√©n√©ration
- Gestion propre des interruptions (Ctrl+C)
- Meilleure exp√©rience utilisateur

### 2. Configuration Optimis√©e

- **Tokens par d√©faut** : 1024 (√©quilibr√© entre vitesse et qualit√©)
- **Max tokens dynamique** : 500-2048 selon personnalit√© et curiosit√©
- **Timeout** : 120 secondes pour √©viter les blocages

### 3. M√©moire et Contexte Pr√©serv√©s

- **Contexte complet** : Tous les messages de la session sont conserv√©s
- **Souvenirs** : 2-4 souvenirs r√©cup√©r√©s selon l'√©tat √©motionnel
- **Apprentissage continu** : Aucune limitation artificielle de la compr√©hension

## ‚öôÔ∏è Configuration

L'IA est configur√©e pour **apprendre en continu sans limite** :

```python
# Configuration actuelle (optimis√©e pour apprentissage)
DEFAULT_MAX_TOKENS = 1024  # R√©ponses compl√®tes
MAX_SHORT_TERM_MEMORY = 20  # Contexte complet pr√©serv√©
MEMORY_RETRIEVAL_K = 5  # Souvenirs pertinents r√©cup√©r√©s
```

**Note** : Si vous voulez sacrifier l'apprentissage pour la vitesse, vous pouvez r√©duire ces valeurs, mais cela limitera la capacit√© de l'IA √† apprendre et √† se souvenir.

## üöÄ Conseils pour de Meilleures Performances (Sans Limiter l'Apprentissage)

### 1. Utiliser un Mod√®le Plus L√©ger

```bash
# Mod√®les plus rapides (mais moins performants)
ollama pull llama3:8b      # Version 8B au lieu de 70B
ollama pull mistral:7b
ollama pull qwen:7b
```

Puis dans `config.py` :
```python
DEFAULT_MODEL = "llama3:8b"
```

### 2. Optimiser le Mat√©riel

- Utiliser un GPU si disponible (Ollama le d√©tecte automatiquement)
- Augmenter la RAM disponible pour Ollama
- Fermer les autres applications gourmandes

### 3. Ajuster la Temp√©rature (Optionnel)

Temp√©rature plus basse = r√©ponses plus d√©terministes = l√©g√®rement plus rapides

```python
DEFAULT_TEMPERATURE = 0.6  # Au lieu de 0.7 (l√©g√®re r√©duction)
```

**‚ö†Ô∏è Important** : Ne r√©duisez pas la m√©moire ou le contexte si vous voulez que l'IA apprenne en continu !

## üìä Temps de R√©ponse Typiques

Avec contexte complet et m√©moire pr√©serv√©e :

- **Mod√®le 7B-8B** : 3-8 secondes
- **Mod√®le 13B** : 8-15 secondes
- **Mod√®le 70B** : 15-45 secondes

**Note** : Ces temps peuvent varier selon la longueur du contexte et le nombre de souvenirs r√©cup√©r√©s. C'est normal et n√©cessaire pour un apprentissage continu.

## üîß D√©pannage

### L'IA est toujours lente

1. V√©rifiez le mod√®le utilis√© : `ollama list`
2. Utilisez une version plus petite (8B au lieu de 70B)
3. V√©rifiez que votre GPU est utilis√© si disponible
4. Augmentez la RAM disponible

### Timeout fr√©quent

Augmentez dans `config.py` :
```python
OLLAMA_TIMEOUT = 180  # 3 minutes pour les mod√®les plus lourds
```

### R√©ponses trop courtes

Augmentez dans `config.py` :
```python
DEFAULT_MAX_TOKENS = 1536  # R√©ponses plus longues
```

### ‚ö†Ô∏è Ne Sacrifiez Pas l'Apprentissage

Si vous r√©duisez la m√©moire ou le contexte pour la vitesse, l'IA perdra sa capacit√© √† :
- Apprendre en continu
- Se souvenir des interactions pr√©c√©dentes
- D√©velopper une compr√©hension cumulative

**Recommandation** : Utilisez un mod√®le plus l√©ger plut√¥t que de limiter la m√©moire.

---

**üí° Philosophie** : L'objectif est un apprentissage continu sans limite, pas la vitesse maximale. La patience permet une IA plus intelligente et plus m√©morielle.
